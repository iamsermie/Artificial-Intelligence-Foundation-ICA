{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solve enviroment without reinforcement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1000 completed\n",
      "Episode 2000 completed\n",
      "Episode 3000 completed\n",
      "Episode 4000 completed\n",
      "Episode 5000 completed\n",
      "Episode 6000 completed\n",
      "Episode 7000 completed\n",
      "Episode 8000 completed\n",
      "Episode 9000 completed\n",
      "Episode 10000 completed\n",
      "Training completed.\n",
      "Episode 1\n",
      "Finished episode 1 with reward 20\n",
      "Episode 2\n",
      "Finished episode 2 with reward 20\n",
      "Episode 3\n",
      "Finished episode 3 with reward 20\n",
      "Episode 4\n",
      "Finished episode 4 with reward 20\n",
      "Episode 5\n",
      "Finished episode 5 with reward 20\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import time  # For adding delays during rendering\n",
    "\n",
    "# Disable the environment checker to avoid np.bool8 error\n",
    "env = gym.make('Taxi-v3', disable_env_checker=True)\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.9  # Learning rate\n",
    "gamma = 0.95  # Discount factor\n",
    "epsilon = 1.0  # Exploration rate\n",
    "epsilon_decay = 0.9995  # Decay rate for exploration\n",
    "min_epsilon = 0.01  # Minimum exploration rate\n",
    "num_episodes = 10000  # Number of training episodes\n",
    "max_steps = 100  # Maximum steps per episode\n",
    "\n",
    "# Initialize Q-table with zeros\n",
    "q_table = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "\n",
    "# Function to choose an action using epsilon-greedy policy\n",
    "def choose_action(state):\n",
    "    if random.uniform(0, 1) < epsilon:\n",
    "        return env.action_space.sample()  # Explore: choose a random action\n",
    "    else:\n",
    "        return np.argmax(q_table[state, :])  # Exploit: choose the best known action\n",
    "\n",
    "# Training process\n",
    "for episode in range(num_episodes):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        action = choose_action(state)\n",
    "        \n",
    "        # Gym 0.26+ returns 5 values: next_state, reward, terminated, truncated, info\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        # Update Q-value using the Bellman equation\n",
    "        old_value = q_table[state, action]\n",
    "        next_max = np.max(q_table[next_state, :])\n",
    "        q_table[state, action] = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # Decay epsilon to reduce exploration over time\n",
    "    epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
    "\n",
    "    # Print progress every 1000 episodes\n",
    "    if (episode + 1) % 1000 == 0:\n",
    "        print(f\"Episode {episode + 1} completed\")\n",
    "\n",
    "print(\"Training completed.\")\n",
    "\n",
    "# Create environment for rendering (also disable checker here)\n",
    "env = gym.make('Taxi-v3', render_mode='human', disable_env_checker=True)\n",
    "\n",
    "# Evaluation\n",
    "for episode in range(5):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "\n",
    "    print(f'Episode {episode + 1}')\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        action = np.argmax(q_table[state, :])  # Always choose the best action\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        state = next_state\n",
    "\n",
    "        time.sleep(2)  # Add a small delay to see each step\n",
    "\n",
    "        if done:\n",
    "            print(f'Finished episode {episode + 1} with reward {reward}')\n",
    "            time.sleep(1)  # Pause before the next episode starts\n",
    "            break\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
